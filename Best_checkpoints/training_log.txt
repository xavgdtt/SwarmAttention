step 0: train loss 4.1830, val loss 4.1796, time 56.98 s
step 200: train loss 1.7432, val loss 1.8763, time 235.38 s
step 400: train loss 1.5300, val loss 1.7086, time 414.00 s
step 600: train loss 1.4439, val loss 1.6350, time 591.62 s
step 800: train loss 1.3824, val loss 1.5852, time 770.59 s
step 1000: train loss 1.3347, val loss 1.5486, time 948.02 s
step 1200: train loss 1.2957, val loss 1.5191, time 1125.11 s
step 1400: train loss 1.2693, val loss 1.5124, time 1302.26 s
step 1600: train loss 1.2372, val loss 1.4998, time 1479.56 s
step 1800: train loss 1.2140, val loss 1.4867, time 1656.87 s
step 2000: train loss 1.1868, val loss 1.4776, time 1834.34 s
step 2200: train loss 1.1682, val loss 1.4703, time 2010.84 s
step 2400: train loss 1.1450, val loss 1.4675, time 2187.73 s
step 2600: train loss 1.1291, val loss 1.4668, time 2365.30 s
step 2800: train loss 1.1125, val loss 1.4726, time 2541.83 s
step 2999: train loss 1.0918, val loss 1.4652, time 2718.09 s

Model Hyperparameters (config):
out_dir: swarmAtt_checkpoints
always_save_checkpoint: False
learning_rate: 0.0002
max_iters: 3000
weight_decay: 0.01
beta1: 0.9
beta2: 0.999
grad_clip: 0.0
dropout: 0.2
eval_iters: 200
eval_interval: 200
batch_size: 64
block_size: 256
n_embd: 256
n_head: 8
n_layer: 14
head_steps: 1
