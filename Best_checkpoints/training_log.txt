step 0: train loss 4.1835, val loss 4.1822, time 77.06 s
step 200: train loss 1.7424, val loss 1.8760, time 295.01 s
step 400: train loss 1.5339, val loss 1.7211, time 512.67 s
step 600: train loss 1.4451, val loss 1.6364, time 727.89 s
step 800: train loss 1.3859, val loss 1.5845, time 943.40 s
step 1000: train loss 1.3404, val loss 1.5487, time 1128.77 s
step 1200: train loss 1.3025, val loss 1.5217, time 1310.17 s
step 1400: train loss 1.2701, val loss 1.5075, time 1489.38 s
step 1600: train loss 1.2420, val loss 1.4967, time 1664.70 s
step 1800: train loss 1.2186, val loss 1.4826, time 1839.95 s
step 2000: train loss 1.1926, val loss 1.4836, time 2014.30 s
step 2200: train loss 1.1717, val loss 1.4646, time 2190.67 s
step 2400: train loss 1.1515, val loss 1.4670, time 2365.08 s
step 2600: train loss 1.1315, val loss 1.4663, time 2540.93 s
step 2800: train loss 1.1175, val loss 1.4783, time 2720.07 s
step 2999: train loss 1.0963, val loss 1.4744, time 2894.52 s

Model Hyperparameters (config):
out_dir: swarmAtt_checkpoints
always_save_checkpoint: False
learning_rate: 0.0002
max_iters: 3000
weight_decay: 0.01
beta1: 0.9
beta2: 0.999
grad_clip: 0.0
dropout: 0.2
eval_iters: 200
eval_interval: 200
batch_size: 64
block_size: 256
n_embd: 256
n_head: 8
n_layer: 14
inf_steps: [1, 2, 3, 4, 1, 2, 3, 4]
head_steps: 1
